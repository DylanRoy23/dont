import os
import numpy as np
from stable_baselines3 import SAC, PPO, A2C
from stable_baselines3.common.callbacks import BaseCallback
from rich.console import Console
from rich.panel import Panel

from gym_env import MultiUAVEnv
from flight_engine.simulator import FixedWingAircraft
from flight_engine.helpers import Position

console = Console()

# ============================================================
# ALGORITHM REGISTRY
# Hyperparameter keys each algorithm reads from config.
# Only SAC has a replay buffer, so buffer_clear only runs for it.
# ============================================================
ALGO_REGISTRY = {
    "SAC": {
        "cls": SAC,
        "has_replay_buffer": True,
        "kwargs": lambda c: dict(
            learning_rate=float(c["learning_rate"]),
            buffer_size=int(c["buffer_size"]),
            learning_starts=int(c["learning_starts"]),
            batch_size=int(c["batch_size"]),
            tau=float(c["tau"]),
            gamma=float(c["gamma"]),
            ent_coef=c["ent_coef"],
            target_update_interval=int(c["target_update_interval"]),
            train_freq=(int(c["train_freq"]), "step"),
            gradient_steps=int(c["gradient_steps"]),
            policy_kwargs=c["policy_kwargs"],
            tensorboard_log=c["tensorboard_log"],
            verbose=c["verbose"],
            device=c["device"],
        ),
    },
    "PPO": {
        "cls": PPO,
        "has_replay_buffer": False,
        "kwargs": lambda c: dict(
            learning_rate=float(c["learning_rate"]),
            n_steps=int(c["n_steps"]),
            batch_size=int(c["batch_size"]),
            n_epochs=int(c["n_epochs"]),
            gamma=float(c["gamma"]),
            gae_lambda=float(c["gae_lambda"]),
            clip_range=float(c["clip_range"]),
            ent_coef=float(c["ent_coef"]),
            vf_coef=float(c["vf_coef"]),
            max_grad_norm=float(c["max_grad_norm"]),
            policy_kwargs=c["policy_kwargs"],
            tensorboard_log=c["tensorboard_log"],
            verbose=c["verbose"],
            device=c["device"],
        ),
    },
    "A2C": {
        "cls": A2C,
        "has_replay_buffer": False,
        "kwargs": lambda c: dict(
            learning_rate=float(c["learning_rate"]),
            n_steps=int(c["n_steps"]),
            gamma=float(c["gamma"]),
            gae_lambda=float(c["gae_lambda"]),
            ent_coef=float(c["ent_coef"]),
            vf_coef=float(c["vf_coef"]),
            max_grad_norm=float(c["max_grad_norm"]),
            policy_kwargs=c["policy_kwargs"],
            tensorboard_log=c["tensorboard_log"],
            verbose=c["verbose"],
            device=c["device"],
        ),
    },
}


class RobustCurriculumCallback(BaseCallback):
    def __init__(self, origin, config, algo_name):
        super().__init__()
        self.origin = origin
        self.config = config
        self.algo_name = algo_name.upper()
        self.change_freq = config["train"]["change_frequency"]
        self.total_timesteps = config["train"]["total_timesteps"]
        self.save_dir = config["train"]["save_dir"]
        self.curriculum = self._build_curriculum(config)
        self.buffer_clear_fraction = config["train"].get("buffer_clear_fraction", 0.5)
        self.current_phase_idx = 0
        os.makedirs(self.save_dir, exist_ok=True)

    def _build_curriculum(self, config):
        c = []

        for phase, args in config["train"]["curriculum"].items():
            c.append((
                float(phase),
                float(args["min_box_size"]),
                float(args["max_box_size"]),
                int(args["num_drones"]),
            ))

        c.sort(key=lambda x: x[0])
        return c


    def _get_curriculum_phase_idx(self):
        """
        Calculates the current phase index based on the progress of the 
        training.

        Args:
            None

        Returns:
            int: The current phase index
        """
        progress = self.num_timesteps / self.total_timesteps
        idx = 0
        for i, phase in enumerate(self.curriculum):
            if progress >= phase[0]:
                idx = i
        return idx

    def _get_curriculum_phase(self):
        """
        Returns the current phase of the curriculum based on the progress of 
        the training.

        Returns:
            tuple: The current phase containing the phase, minimum box size, 
            maximum box size, and number of drones.
        """
        return self.curriculum[self._get_curriculum_phase_idx()]

    def _generate_new_box(self):
        """
        Generates a new bounding box for the environment based on the current 
        phase.

        The bounding box is a rectangle with its center at the origin and its 
        size randomly sampled from the minimum and maximum box sizes for the 
        current phase.

        Returns:
            tuple: A tuple containing the top-left and bottom-right 
                coordinates of the bounding box in latitude-longitude format, 
                and the width and height of the box in meters.
        """
        _, min_m, max_m, _ = self._get_curriculum_phase()

        w_m, h_m = np.random.uniform(min_m, max_m, size=2)

        lat_off = (h_m / 2.0) / 111_320.0
        lon_off = (w_m / 2.0) / (
            111_320.0 * np.cos(
            np.radians(
                self.origin[0]
                )
            )
        )

        tl = (
            self.origin[0] + lat_off, 
            self.origin[1] - lon_off
        )
        br = (
            self.origin[0] - lat_off, 
            self.origin[1] + lon_off
        )

        return tl, br, w_m, h_m

    def _add_drone(self):
        """
        Adds a new drone to the environment at the origin with a random 
        heading.
        """
        env = self.training_env.envs[0].unwrapped
        new_id = f"UAV-{len(env.aircraft_list) + 1}"

        heading = np.random.uniform(-np.pi, np.pi)

        new_uav = FixedWingAircraft(
            new_id,
            Position(self.origin[0], self.origin[1]),
            heading,
            self.config["train"]["drone_speed"],
            self.config["train"]["drone_turn_rate"],
            speed_variance = self.config["train"]["speed_var"], 
            turning_variance = self.config["train"]["speed_var"]
        )

        env.aircraft_list.append(new_uav)

    def _update_drone_count(self):
        """
        Updates the number of drones in the environment to match the current 
        phase of the curriculum.
        """
        _, _, _, target_uavs = self._get_curriculum_phase()
        env = self.training_env.envs[0].unwrapped

        while len(env.aircraft_list) < target_uavs:
            self._add_drone()
            console.print(
                Panel(
                    f"[bold cyan]UAV ADDED[/bold cyan]\n"
                    f"Total UAVs: {len(env.aircraft_list)}",
                    expand=False,
                )
            )

    def _partial_buffer_clear(self):
        """
        SAC-only: partially clears the replay buffer at phase transitions
        to reduce stale experience from earlier curriculum phases.
        Skipped automatically for PPO and A2C (no replay buffer).
        """
        buffer = self.model.replay_buffer
        if buffer.full or buffer.pos > 0:
            keep_count = int(buffer.pos * (1.0 - self.buffer_clear_fraction))
            if keep_count > 0 and buffer.pos > keep_count:
                start = buffer.pos - keep_count
                for attr in ['observations', 'actions', 'rewards', 'dones', 'next_observations']:
                    data = getattr(buffer, attr, None)
                    if data is not None:
                        data[:keep_count] = data[start:buffer.pos]
                buffer.pos = keep_count
                buffer.full = False
            elif keep_count == 0:
                buffer.reset()

            console.print(
                Panel(
                    f"[bold yellow]BUFFER PRUNED[/bold yellow]\n"
                    f"Cleared {self.buffer_clear_fraction*100:.0f}% of replay buffer\n"
                    f"Remaining transitions: {buffer.pos}",
                    expand=False,
                )
            )

    def _on_step(self) -> bool:
        phase_idx = self._get_curriculum_phase_idx()

        if phase_idx > self.current_phase_idx:
            self.current_phase_idx = phase_idx
            phase = self.curriculum[phase_idx]
            algo_prefix = self.algo_name.lower()
            save_path = os.path.join(
                self.save_dir,
                f"{algo_prefix}_phase_{phase_idx}_step_{self.num_timesteps}"
            )
            self.model.save(save_path)

            console.print(
                Panel(
                    f"[bold magenta]MODEL SAVED[/bold magenta]\n"
                    f"Algorithm: {self.algo_name}\n"
                    f"Phase: {phase_idx}\n"
                    f"Progress ≥ {phase[0]*100:.0f}%\n"
                    f"Saved to:\n{save_path}",
                    expand=False,
                )
            )

            # Only SAC has a replay buffer to clear
            if ALGO_REGISTRY[self.algo_name]["has_replay_buffer"]:
                self._partial_buffer_clear()

        if self.num_timesteps % self.change_freq == 0:
            tl, br, w, h = self._generate_new_box()
            self.training_env.env_method("update_bounds", tl, br)

            _, _, _, uavs = self._get_curriculum_phase()

            console.print(
                Panel(
                    f"[bold green]Curriculum Update[/bold green]\n"
                    f"Step: {self.num_timesteps}\n"
                    f"Area: {w:.0f}m × {h:.0f}m\n"
                    f"UAVs: {uavs}",
                    expand=False,
                )
            )

        self._update_drone_count()
        return True

def train(config):
    algo_name = config["train"].get("algorithm", "SAC").upper()

    if algo_name not in ALGO_REGISTRY:
        raise ValueError(f"Unknown algorithm '{algo_name}'. Choose from: {list(ALGO_REGISTRY.keys())}")

    algo_info = ALGO_REGISTRY[algo_name]

    console.print(
        Panel.fit(
            f"[bold white]Multi-UAV {algo_name} Trainer[/bold white]",
            subtitle="Percent-Based Curriculum Learning",
        )
    )

    # Random global origin
    origin = (
        np.random.uniform(-70.0, 70.0),
        np.random.uniform(-170.0, 170.0),
    )

    # Initial UAV
    initial_heading = np.random.uniform(-np.pi, np.pi)
    initial_uavs = [
        FixedWingAircraft(
            "UAV-1",
            Position(origin[0], origin[1]),
            initial_heading,
            config["train"]["drone_speed"],
            config["train"]["drone_turn_rate"],
            speed_variance = config["train"]["speed_var"], 
            turning_variance = config["train"]["speed_var"]
        )
    ]

    # Initial small training box
    w_m = h_m = config["train"]["curriculum"][0.0]["min_box_size"]
    
    lat_off = (h_m / 2.0) / 111_320.0
    lon_off = (w_m / 2.0) / (111_320.0 * np.cos(np.radians(origin[0])))

    tl = (origin[0] + lat_off, origin[1] - lon_off)
    br = (origin[0] - lat_off, origin[1] + lon_off)

    # dt: Time step size for physics simulation 
    # (smaller values yield more realistic simulation)
    env = MultiUAVEnv(
        initial_uavs, 
        tl=tl, 
        br=br, 
        dt=config["train"]["dt"],
        max_steps=config["train"]["max_steps"],
        boundary_margin=config["train"]["boundary_margin"],
        mission_waypoint_count=config["train"]["mission_waypoint_count"],
        mode='gen_mission', #gen_mission or manual_mission
        caution_dist=config["train"]["caution_dist"],
        critical_dist=config["train"]["critical_dist"]
    ) 

    model = algo_info["cls"](
        "MlpPolicy",
        env,
        **algo_info["kwargs"](config["train"])
    )

    callback = RobustCurriculumCallback(
        origin=origin,
        config=config,
        algo_name=algo_name
    )

    try:
        model.learn(
            total_timesteps=config["train"]["total_timesteps"],
            callback=callback,
            progress_bar=True,
            tb_log_name=config["train"]["model_name"],
        )
        model.save(
            os.path.join(
                config["train"]["save_dir"], 
                config["train"]["model_name"]
            )
        )
    except KeyboardInterrupt:
        model.save(
            os.path.join(
                config["train"]["save_dir"], 
                f"{config['train']['model_name']}_interrupted"
            )
        )

if __name__ == "__main__":
    print()
